
# Idea2Paper â€” Upload â†’ arXiv â†’ SPECTER â†’ PEGASUS (non-LLM scientific drafts)


A lightweight pipeline that turns your own notes/experiment logs into a short, researchâ€‘style draft by:
- extracting **keywords** from your upload,
- pulling **related arXiv papers**,
- ranking them with **SPECTER** (semantic similarity),
- and summarizing the background with **PEGASUSâ€‘arXiv** (abstracts or fullâ€‘PDFs).

No paid LLM keys required; models run locally via Hugging Face + PyTorch. Works on **CPU** or **CUDA** if available.

---

## âœ¨ Pipeline

## Pipeline (abstract)

```mermaid
flowchart TD
  A[Upload notes / draft] --> B[Keyword Extraction]
  B --> C[arXiv Retrieval]
  C --> D[SPECTER Ranking]
  D --> E[Summarize Background]
  E --> F[Compose Research-style Draft]
  F --> G[Markdown Output]

  %% optional helper (comment out if not wanted)
  A -. optional .-> X[Light edits / clarifications]
  X -. feeds .-> B
```

*This diagram intentionally keeps the flow abstract and independent of any specific PDF/fullâ€‘text step.*

---

## ğŸš€ Quickstart (Windows)

```powershell
git clone https://github.com/<your-username>/Idea2Paper.git
cd Idea2Paper

# Create & activate venv
python -m venv .venv
. .venv/Scripts/activate

# Install deps
pip install --upgrade pip
pip install -r requirements.txt

# Run the Streamlit app
streamlit run src/app.py
```

### GPU (optional)
If you have an NVIDIA GPU, CUDA Toolkit/Driver installed:
- The app autoâ€‘detects CUDA. You can force it via env var:
  ```powershell
  set IDEA2PAPER_DEVICE=cuda
  ```
- Models will use **fp16** on GPU to save VRAM and speed up generation.

> **Torch vulnerability notice:** Some Transformers checkpoints require `torch>=2.6` (or safetensors) because of a security fix (CVEâ€‘2025â€‘32434). If you hit a â€œplease upgrade torchâ€ error, use:
> ```powershell
> pip install --upgrade "torch>=2.6" --index-url https://download.pytorch.org/whl/cu121  # choose cu version to match your CUDA
> ```

---

## ğŸ—‚ï¸ Repository Layout

```
Idea2Paper/
â”œâ”€ flowchart/                  # Mermaid diagram(s) for the README
â”‚  â””â”€ pipeline.md
â”œâ”€ data_samples/               # (Empty in repo; add your own test files) 
â”œâ”€ drafts/                     # Generated drafts (git-ignored)
â”œâ”€ notebook/
â”‚  â””â”€ demo.ipynb               # Optional smoke tests / quick experiments
â”œâ”€ src/
â”‚  â”œâ”€ app.py                   # Streamlit UI
â”‚  â”œâ”€ config.py                # Central configuration
â”‚  â”œâ”€ ingest.py                # Upload parsing (pdf/docx/txt/md/csv)
â”‚  â”œâ”€ keywords.py              # Keyphrase extraction
â”‚  â”œâ”€ retrieval.py             # arXiv query builder + fetch
â”‚  â”œâ”€ ranker.py                # SPECTER-based re-ranking
â”‚  â”œâ”€ summarizer.py            # PEGASUS summarization (abstracts / full-PDF)
â”‚  â”œâ”€ generator.py             # Compose Markdown draft
â”‚  â”œâ”€ utils.py                 # Helpers
â”‚  â””â”€ fulltext.py              # (Optional) PDF extraction helper
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## âš™ï¸ Config (important knobs)

All tunable via `src/config.py` or environment variables:

- `DEVICE` â€” `"cuda"` or `"cpu"` (auto-detected; override with `IDEA2PAPER_DEVICE`)
- `TOP_K` â€” how many top ranked papers to use in the draft
- `MAX_ARXIV_RESULTS` â€” how many raw arXiv results to fetch before ranking
- `USE_FULL_PDFS` â€” `true/false` (if `true`, attempt to download & summarize full PDFs)
- `PDF_TIMEOUT`, `PDF_MAX_BYTES`, `PDF_CHAR_CAP` â€” safety rails for PDF downloads
- `SUM_MODEL` â€” default summarizer (`google/pegasus-arxiv`)
- `EMBED_MODEL` â€” SPECTER encoder (`sentence-transformers/allenai-specter`)

---

## âœ… What works well

- No API keys needed; local semantic ranking + summarization
- RAG based on your upload + arXiv related work
- Simple UI that makes it easy to iterate and save drafts

## ğŸ˜• What disappointed me

- The generated â€œpaperâ€ is concise and **not** as rich as LLMâ€‘assisted drafts.
- Quality depends heavily on upload clarity and arXiv retrieval.
- Fullâ€‘PDF extraction can be slow/unreliable depending on source PDFs.

## ğŸ§­ Roadmap / ideas to improve

- Swap to a **local instructionâ€‘tuned LLM** (Ollama) for drafting & section structure
- **Citation scraping** and reference formatting (BibTeX export)
- Domainâ€‘specific rankers (SciBERT, multiâ€‘query retrieval, category filters)
- Smarter **hierarchical longâ€‘context** summarization across multiple PDFs with chunk memory
- Better PDF parsing (layoutâ€‘aware extraction / figure caption mining)

---

## ğŸ§° Troubleshooting

- **CUDA found but slow?** Some models still do heavy tokenization on CPU; thatâ€™s expected. You can still speed up generation by using fp16 on GPU.
- **Transformers says â€œupgrade torchâ€?** Install `torch>=2.6` or switch to safetensorsâ€‘only models.
- **arXiv returns zero results?** Tweak keywords; try fewer/more general terms.

---

## ğŸ¤ Credit

No license file right now. If you use this repo in your work or demos, a short credit like
> â€œBuilt on *Idea2Paper* (https://github.com/Sivarohitk/Idea2Paper)â€  
is highly appreciated.
